# 用于在线哈希的快速类更新

Mingbao Lin, Rongrong Ji*, *Senior Member, IEEE,* Xiaoshuai Sun, Baochang Zhang, Feiyue Huang, Yonghong Tian, *Senior Member, IEEE,* and Dacheng Tao, *Fellow,* IEEE

[TOC]



## 摘要

​		在线图像哈希最近受到越来越多的研究关注，它以流式方式处理大规模数据在运行中更新哈希函数。为此，大多数现有工作在有监督的背景下利用这个问题，即使用类标签来提高哈希性能，这在适应性和效率方面都存在缺陷：首先，需要大量的训练批次来学习最新的哈希函数，这导致了在线自适应性差。其次，训练是耗时的，这与在线学习的核心需求相矛盾。本文提出了一种新的有监督在线哈希方案，称为快速类更新在线哈希算法（FCOH），通过引入一种新颖高效的内积运算来解决上述两个难题。为了实现快速在线自适应，提出了一种基于类的更新方法来分解二进制代码学习，并以类的方式交替更新哈希函数，很好地解决了大量批训练的负担。从数量上讲，这种分解进一步导致至少75%的存储节省。为了进一步实现在线高效率，我们提出了一种半松弛优化方法，该方法通过独立处理不同的二进制约束来加速在线训练。在没有附加约束和变量的情况下，时间复杂度显著降低。这种方案也被定量地显示为在更新哈希函数期间很好地保留了过去的信息。我们已经定量证明，与各种最先进的方法相比，按类别更新和半松弛优化的共同努力提供了优越的性能，这通过对三个广泛使用的数据集上的大量实验得到了验证。

**索引词**—图像检索，相似性保持，在线哈希，二进制代码。

## 1 前言

​		传统的哈希方法[1]、[2]、[3]、[4]、[5]、[6]、[7]、[8]、[9]、[10]、[11]、[12]、[13]、[14]、[15]、[16]、[17]、[18]、[19]、[20]、[21]、[22]、[23]大多设计用于从有/无监督标签的固定训练数据集合中离线学习哈希函数。但是，这样的设置无法处理数据以流方式输入系统的动态应用程序场景。因此，在线哈希最近引起了很多研究关注[24]、[25]、[26]、[27]、[28]、[29]、[30]、[31]、[32]、[33]、[34]、[35]、[36]、[37]。它的目标是使用顺序到达的数据实例在线更新哈希函数，这在大规模在线检索应用中具有优越的适应性和可扩展性。

​		理想情况下，在线哈希应该基于运行中的流数据高效地更新哈希函数，同时保留过去数据流中的信息。现有的在线哈希算法可分为有监督和无监督两类。代表性的监督方法包括但不限于OKH[24]、AdaptHash[27]、OSH[30]、MIHash[33]、HCOH[35]和BSODH[36]，它们使用监督标签指导在线哈希学习。与此相反，无监督的在线哈希方法，例如，SketchHash[26]和FROSH[ 34 ]考虑从大数据集[38]中的较小的数据梗概来更新哈希函数，同时保留数据集的主要属性。由于标签信息的使用，有监督的在线哈希方法通常比无监督的方法产生更好的结果，因此这是文献中的主要趋势，也是本文的重点。

​		![image-20211201161406852](..//note/picture/image-20211201161406852.png)

**图1**.现有的有监督在线哈希方法[24]、[26]、[27]、[30]、[33]、[34]、[35]、[36]与我们的方法在适应性方面的比较。现有的方法只有在最后的训练阶段（实心矩形框）才能达到最佳性能，无法在线快速调整，需要大量训练批次来学习最新的哈希函数（虚线矩形框）。与之不同的是，所提出的FCOH方法可以在早期阶段以更少的训练数据在线获得更好的自适应性。

​	然而，由于两个问题，有监督的在线哈希仍然是一个开放的问题，如第3节验证的一样。首先，需要大量的训练批次来学习最新的哈希函数，导致了在线自适应性差的问题。以前的工作[24]、[26]、[27]、[30]、[33]、[34]、[35]、[36]只关注于设计哈希模型，以在最终训练阶段获得完整的性能，因此缺乏足够的自适应性，如图1所示。相比之下，这些方法在早期训练阶段表现相对较差。第二，最先进的监督方法更新每个传入数据流大多是很耗时的[33]、[35]、[36]。特别是，给定一个查询，MIHash[33]必须计算其邻居和非邻居之间的汉明距离来更新哈希函数，这不可避免地增加了时间消耗的负担。在[35]中，预定义的ECOC码本[39]用于指导哈希函数的学习，从而消除了对相似性保持的强约束。然而，由于使用LSH来保持码长的一致性和ECOC码本的大小，短哈希码的质量仍然性能不好。[36]中的工作考虑了基于内积的方案，该方案采用两个平衡因子来解决“数据不平衡”问题，并允许在在线设置中使用离散优化[9]。然而，一方面，它引入了更多需要优化的变量。另一方面，离散优化的使用面临着收敛性问题。这两个缺陷不可避免地增加了训练负担。OSH[30]采用在线Boosting算法[40]来增强在线学习能力。然而，boosting的使用不可避免地导致培训效率低下。值得注意的是，早期的有监督方法如OKH[24]和AdaptiveHash[27]以及无监督方法如SketchHash[26]和FROSH[34]具有很高的训练效率，但它们的性能远远不能令人满意，随着训练数据的增加，训练效率会变得更差（如图1所示，随后在第4.2节中验证）。

![image-20211201163253443](..//note/picture/image-20211201163253443.png)

**图2**提出的类更新的图解。对于到达的流批次，更新被划分为几个独立的子流程，即（a）、（b）、（c）和（d）。在每个子流程中，FCOH旨在区分特定类别和其他类别，这是按顺序进行的$((a)\rightarrow(b)\rightarrow(c)\rightarrow(d))$.由此，邻域信息被很好地嵌入到汉明空间中。

​		为了解决上述问题，在本文中，我们提出了一种新的有监督在线哈希方法，称为在线哈希快速类更新（FCOH）。我们的关键创新是引入内积来保持汉明空间中的相似关系，这在离线学习二进制代码[12]、[36]、[41]、[42]、[43]时被证明是非常有效的。与以前基于内积的在线哈希不同[36]，我们在一个统一的框架中解决了在线适应性和训练效率方面的缺陷。在此框架中，我们首先开发了一种新的更新方案，该方案以类方式交替地更新哈希函数，其优点如图2所示。在每一轮中，我们只更新一个类中的样本，这些样本可以很好的从其他类中区别出来。因此，哈希函数可以用更少的训练数据在线快速调整。这种类更新方案不同于以前的工作，以前的工作需要训练中涉及的所有数据，因此避免了大量内存消耗。从数量上讲，这种基于内积的类更新可以显著降低至少75%的空间复杂度。其次，为了加速在线训练，我们提出了一种基于分治法求解二元约束的半松弛优化方法，即将部分二元约束松弛为连续约束，而将其余部分视为常数，可以通过在前一阶段中学习的哈希权重预先计算。这样，就不需要额外的约束和变量，大大降低了训练的复杂性。因此，所提出的FCOH具有更高的可扩展性和更高的效率，并且可以很好地应用于大规模应用中，其复杂度比以前的工作要低。此外，我们还证明了半松弛优化能够很好地保留过去的信息，从而进一步提高了性能。

我们FCOH方案的主要贡献包括：

- 在汉明空间中引入内积的概念来保持相似关系。与以往基于内积的在线方法不同，该框架的关键创新在于在一个统一的框架中解决在线适应性和训练效率问题。
- 我们开发了一种基于类的更新方案，在在线哈希中以更少的训练数据和存储消耗有效地更新哈希函数。与传统的在线哈希方法不同，我们的方法可以在线快速自适应，很好地解决了在线自适应问题。
- 我们设计了一种新的半松弛优化方法，大大缩短了内积法的训练时间。我们的方法基于分治优化来解决二进制约束，通过将部分二进制约束放松为连续约束。其余部分被表示为一个常数，可以通过在上一阶段学习的哈希权重预先计算，这很好地保留了过去的信息。

​         所提出的类更新和半松弛优化的共同努力极大地提高了在线哈希的性能。在三个广泛使用的基准（即CIFAR-10、Places205和MNIST）上进行的大量实验表明，所提出的FCOH比最先进的方法[24]、[26]、[27]、[30]、[33]、[34]、[35]、[36]具有更高的精度和效率。

​		本文的其余部分组织如下：第2节讨论相关工作。提出的包括类更新和半松弛优化的FCOH算法详细说明见第3节。第4节报告了实验结果。最后，我们在第5节中总结了这项工作。

## 2 相关工作

​		有监督的在线哈希方法利用标签信息学习二进制代码。据我们所知，在线核哈希（OKH）[24]是此类的第一种方法，它需要点对通过在线被动侵略策略更新哈希函数[44]。自适应哈希（AdaptiveHash）[27]定义了一个hinge-like损失，该损失由一个可微的Sigmoid函数近似，用随机梯度下降法（SGD）更新哈希函数。在[30]中，引入了一种更通用的两步哈希法，其中首先将二进制纠错输出码（ECOC）[45]、[46]、[47]分配给标记数据，然后使用在线boosting学习哈希函数以适应二进制ECOC。Cakir等人[33]开发了一种在线互信息哈希（MIHash），其目标是优化给定查询的邻居和非邻居之间的互信息。Lin等人[35]提出了一种基于Hadamard码本的在线哈希（HCOH），其中使用更具辨别力的Hadamard矩阵[39]作为ECOC码本来指导哈希函数的学习。最近，为了在线离散哈希的平衡相似性（BSODH）[36]，被开发以通过内积方式研究新数据和现有数据集之间的相关性。为了解决“数据不平衡”问题，BSODH采用了两个平衡因子来平衡相似度矩阵，并支持离散优化[9]在在线学习中的应用。尽管在有监督的在线哈希[24]、[27]、[30]、[33]、[35]、[36]方面已经取得了广泛的进展，但仍然存在两个关键问题：在线适应性差和训练效率低，如第1节所述.

​		无监督在线哈希主要基于“数据草图”[38]的思想设计，其中使用一小组草图数据来保留大规模数据集的主要属性。为此，Leng等人[26]提出了一种在线草图哈希（SketchHash），它利用奇异值分解（SVD）的一种有效变体来学习哈希函数，并在草图上基于主成分分析（PCA）的批量学习来学习哈希权重。[34]中开发了一种更快的在线草图哈希（FROSH），其中在不同的数据块上使用了独立的二次抽样随机Hadamard变换（SRHT），以使草图更加紧凑和准确，并进一步加快学习过程。一般来说，无监督的在线哈希由于缺乏监督标签而性能低下。

## 3 提出的框架

### 3.1 问题定义

假设数据集是n个向量集合的形式，$X=[x_1，…，x_n]\in R^{d×n}$ ，以及类标签的集合$L=[l_1，…，l_n]∈ N^n$。此外，我们将$C=|set（L）|$表示为L的总类别。我们的目标是学习一组r-bit哈希码$B=[b_1，…，b_n]∈ \{−1，+1\}^{r×n}$，以保存所需的邻域结构。这通过使用r个 哈希函数的集合$H(X) =\{h_i(X)\}^r_{i=1},i.e.$投影数据集$X$来实现.例如：

![image-20211205204612243](../note/picture/image-20211205204612243.png)

其中$W=\{w_i\}^r_{i=1}\in R^{d\times r}$ 是投影矩阵$w_i$是第i个哈希函数。符号函数定义如下：

![image-20211205204852487](../note/picture/image-20211205204852487.png)

在在线环境中，$X$以流式方式出现。因此，对于t阶段的流数据，我们标志$X^t=[x_1^t,...,x^t_{n_t}]\in R^{d\times n_t}$

为输入数据，标记$B^t=[b_1^t,...,b^t_{n_t}]\in \{-1,+1\}^{r\times n_t}$为$X^t$中学习到的二进制代码，标记$L^t=[l_1^t,...,l^t_{n_t}]\in N^{n_t}$为对应的标签集，其中$n_t$是t阶段流数据的大小。进一步，我们标记$C^t=|set(L^t)|$为t阶段接收到的总类别。相应的，参数$W$在t阶段更新被标记为$W^t$。注意到，$W^t$只能在在线环境下通过$X^t$推断得到。

​	在表1。我们总结了下文中会使用到的标记。

表1这篇文章使用到的标记列表

![image-20211205210043492](../note/picture/image-20211205210043492.png)

### 3.2 提出的方法

提出的框架基于基于内积的公式[12]、[36]、[41]、[42]、[43]构建。其关键是将数据点映射成二进制码，二进制码的内积可以很好地逼近相似矩阵$S^t∈ \{−1，+1\}^{n_t×n_t}$，其中如果$l^t_i=l^t_j$$，S^t_{ij}=1$,否则等于-1.更准确地说，基于内积的方法的目标是最小化以下目标函数：
![image-20211205210740014](../note/picture/image-20211205210740014.png)

其中$\Vert \cdot \Vert_F$是范数。

​		上述公式已被证明在传统的离线哈希方法[12]、[41]、[42]、[43]中有效。然而，正如[36]中指出的那样，由于“数据不平衡”问题，直接将该等式应用于在线学习是不可行的。具体地说，在第t阶段，等式（2）可以重写为：

![image-20211205211210951](../note/picture/image-20211205211210951.png)

在在线设置中，相似矩阵$S^t$是高度稀疏的，即，大多数数据对是不同的。因此，在实践中，我们有term B>>term A，这被称为“数据不平衡”问题。由于二进制码的学习在很大程度上依赖于这些不同的码对，检索性能不能达到预期的要求。为了解决这个问题，[36]中的工作引入了两个平衡因子，并允许在在线设置中使用离散优化。然而，这种解决方案需要大量训练批次来学习最新的散列函数，这导致早期训练阶段的在线自适应性差，如图1所示。此外，它还引入了更多需要优化的变量，离散优化进一步带来了收敛问题。这两大弊端不可避免地导致训练效率低下。

我们的方法的新颖之处在于两个方面：首先，为了解决在线自适应性差的问题，我们提出了一种“类更新”方案，该方案将优化分解为几个独立的子流程，每个子流程负责一个类别，从而实现了高精度和至少75%的存储节省，如后面3.2.1小节所示。其次，针对训练效率低下的问题，提出了一种“半松弛优化”方案。在该方案中，等式（2）中的二元约束的一部分作为连续变量重新松弛，而另一部分被视为常数变量，从而大大降低了时间复杂度。下面对以上两种方法进行详细描述。

#### 3.2.1 类更新

为了解决在线自适应性差的问题，我们开发了一种类更新方案，如图2所示。具体而言，我们将等式（3）改写如下：

![image-20211205211923303](../note/picture/image-20211205211923303.png)

其中c表示第t个训练阶段的第c个类。

Term 1表示第c类实例之间的内积，而term 2表示第c类实例与不包括第c类的类实例之间的内积.

​		类更新背后的基本思想是，在第t个训练阶段，我们将训练过程分解为独立于$C_t$的子过程，如图2所示。在第C个子流程中，FCOH专注于学习第c类的判别二进制代码，这可以很好地将第c类的训练实例从其他类的实例中分离出来。图3举例说明了一个小型示例，即类更新通过逐梯度方式更新哈希权重。在总共执行了$C_t$子进程之后，最终的哈希权重为所有类保持了更好的整体性能，并在t阶段快速适应即将到来的数据。

![image-20211205212633511](../note/picture/image-20211205212633511.png)

**图3** 类更新分析。蓝色和绿色曲线表示第一类和第二类的损失。橙色曲线代表总体损失。$W_1^t$是第t个更新阶段的起点。在没有类更新的情况下，$W_1^t$只更新一次，更新的权重为$W_2^t$
，这不是一个第一类和第二类的好选择。通过类更新，哈希权重逐梯度更新。它首先使用$L_{1t}$更新哈希函数并获得$W_3^t$，在此基础上使用$L_{2t}$，然后获得$W^t_4$，作为t级的最终权重，并显示出类别1和类别2的总体较好结果。

​		在不损失泛化性能的情况下，我们分析了同时分解的等式（4）可以大幅降低等式（2）中至少75%的空间复杂度。为此，我们首先在下文中重新表述等式（4）：

![image-20211205213148297](../note/picture/image-20211205213148297.png)

式中，$L_{ct}=term1+term2$ 即等式（4）中的一二两项，表示第c类的损失目标。此外，我们将$L_{ct}$改写为矩阵形式：

![image-20211205213449768](../note/picture/image-20211205213449768.png)

其中，$B^{ct}$是由第c类二进制代码组成的矩阵，$B^{\bar{c}t}$是由不包括第c类的二进制代码组成的矩阵；$n_{ct}$是t阶段第c类的总x训练实例数，$n_{\bar{c}t}$是t阶段不包括第c类的总训练实例数。很容易推导出式（6）中的空间复杂度是$O（n_{ct}n_{\bar{c}t}）$，而不是式（2）中的$O（n^2_t）$。存储节省率R可表示为：

![image-20211205213838965](../note/picture/image-20211205213838965.png)

当$n_{ct}=n_{\bar{c}t}=\frac12n_t$时，$n_{ct}n\bar{ct}=\frac14n^2_t$需要的内存消耗量最大。在这种情况下，提出的的FCOH获得R=75%的存储节省率。当$n_{ct}\neq n_{\bar{c}t}$时，$n_{ct}n\bar{ct}<\frac14n^2_t$且R>75%。因此，提出的FCOH可以减少至少75%的存储消耗。

​		我们注意到早期的工作[48]采用了类似的类更新方案。然而，我们的工作仍然与[48]有明显的区别：首先，[48]解决了离线哈希问题，而FCOH解决了在线哈希问题。其次，[48]解决了基于分类器的离线哈希问题，而FCOH解决了基于内积的在线哈希问题。最后，[48]建议对二进制代码进行分类创新，而FCOH建议对哈希权重进行分类更新。

#### 3.2.2 半松弛优化

由于等式（6）中的离散约束，很容易得出上述问题是高度非凸且难以解决（通常为NP难问题）。另一种次优解决方案是应用[9]中开发的离散循环坐标下降（DCC），即更新一个哈希位，同时固定其他位。然而，如最近的在线哈希方法[36]所示，离散优化带来了更多的变量需要优化，并且逐位方案很难收敛。上述两个问题不可避免地会导致更多的训练负担（如表7所示），这应该在在线哈希中进一步解决。

为了实现有效的优化，我们采用了松弛方法，通过部分去除等式（1）中的sgn（x）函数。然而，与传统的松弛过程不同，在传统松弛过程中，符号函数都从二元约束中移除，我们基于分治思想提出了半松弛式（6）中的符号函数。它去除了二元矩阵内积部分的符号函数，并将二元矩阵的另一部分视为常数。

为此，我们将公式（6）重新表述如下：

![image-20211206091225443](../note/picture/image-20211206091225443.png)

其中

![image-20211206091815350](../note/picture/image-20211206091815350.png)

且

![image-20211206091851950](../note/picture/image-20211206091851950.png)

这是在（t-1）阶段预先计算的两个常数矩阵−1）。$λ_1$和$λ_2$是平衡这两项重要性的两个超参数。

​		这种操作的优点有两个：第一，$B^{c（t−1）}$ 和$B^{\bar{c}（t−1）}$ 由（t-1）个哈希权重提前计算好。因此，不仅在t阶段节省了训练时间，而且从过去的阶段学到了更多的信息。同时，在更新中，项$W^{t−1}X^{ct}$旨在提供当前阶段的新信息。因此，半松弛可以从新的流数据中学习信息，也可以保留上一阶段的信息。其次，与[36]中基于内积的方法相比，这种半松弛方法简化了优化复杂度，即只需优化$W^t$，大大提高了训练效率。

​		然而，式（8）只考虑了当前和上一阶段的线索。随着训练的进行，来自早期阶段的流数据容易遭受较大的量化错误。然而，随着动态数据集的增长，量化所有过去的流化数据非常耗时。此外，它还违反了在线散列的原则，因为散列函数只能根据在线新到达的流数据进行更新。为了解决这个问题，受[49]的启发，我们进一步提出量化易于跟踪的第c类中心，如下所示：

![image-20211206092843753](../note/picture/image-20211206092843753.png)

其中$\Vert\cdot \Vert_1$是$l_1$范数，$|\cdot|$返回绝对值，**1**是单位矩阵并且：

![image-20211206093245948](../note/picture/image-20211206093245948.png)

其中$N_{ct}=N_{c（t−1）} +n_{ct}$和$N_{c1}=n_{c1}$。$N_{ct}$表示第c类的数量，$\bar{x}^{ct}$是第c类的中心。

结合式（8）和式（11）得出如下最终目标函数：

![image-20211206093641573](../note/picture/image-20211206093641573.png)

为了优化等式（13），我们采用SGD优化来更新第c类的哈希函数，如下所示：

![image-20211206093704457](../note/picture/image-20211206093704457.png)

其中µ是学习率。$\bar{L}_{ct}（W^{t−1}）$关于$W^{t-1}$的导数可通过以下方式获得：

![image-20211206093853661](../note/picture/image-20211206093853661.png)

其中σ（x）函数定义为：

![image-20211206094027870](../note/picture/image-20211206094027870.png)

在不丧失一般性的情况下，提出的FCOH的主要程序在Alg1中概述。

![image-20211206094119658](../note/picture/image-20211206094119658.png)

#### 3.3 时间复杂度

在Alg1中，大部分训练时间花费在第11行到第13行之间的操作上，以在t阶段更新第c类上的哈希函数。具体来说，第11行计算$B^{c（t）}$的复杂性 是$O（rdn_{ct}）$，在第12行用于计算$B^{\bar{c}（t−1）}$的复杂度是$O（rdn_{\bar{c}t}）$ 。第13行中计算$\bar{L}_{ct}（W^{t−1}）$ 关于$W^{t−1}$导数的时间成本是$O（rdn_{ct}+rn^2_{ct}+dn^2_{ ct}+rn_{ct}n_{\bar{c}t}+dn_{ct}n\bar{c}t+rdn_{\bar{c}t}+rd）$。
由于$r\ll d$和$n_{ct}\ll n_{\bar{c}t}$，总的复杂性为$O（rdn_{\bar{c}t}+dn_{ct}n_{\bar{c}t}）$。我们表示$e=max（r，n_{ct}）$，复杂度可以重新写为$O（edn_{\bar{c}t}）$。因此，第t个训练阶段的总体时间复杂度为$O（C_tedn_{\bar{c}t}）$。如第4节所示。$C_t、e$和$n_{\bar{c}t}$为小值。因此，该方法的时间复杂度主要取决于特征维数d，这保证了该方法的效率和可扩展性。

## 4 实验

为了验证模型的准确性和学习效率，我们在三个广泛使用的数据集（即CIFAR-10[50]、Places205[51]和MNIST[52]上，将我们的FCOH与几种最先进的方法[24]、[26]、[27]、[30]、[33]、[35]、[36]进行了比较。

### 4.1 实验环境

####  4.1.1 数据集

CIFAR-10由10个类别的60000张图像组成。每个类包含6000个实例，每个图像表示为4096维CNN向量[53]。如[33]、[35]、[36]中所述，我们随机选择59000个样本，形成一个检索集。剩下的1000个用于构建一个测试集。此外，我们从检索集中随机抽取20K图像作为训练集，以流式方式学习哈希函数。
Places205是一个大型数据集[51]，包含205个场景类别的250万张图像。从AlexNet[54]中提取每幅图像的特征，然后通过PCA将其缩减为128维特征。[35]之后，从每个类别中随机抽取20个实例来构建测试集，其他实例用于形成检索集。最后，使用检索集中100K图像的随机子集以流方式学习哈希函数。

MNIST包含70000个从0到9的手写数字图像[52]。每个图像由28×28=784维标准化原始像素表示。根据[36]中的实验设置，我们从每个类中随机抽取100个实例来构建测试集。其余的用于形成检索集。最后，从检索集中随机抽取20000张图像的子集，形成训练集，以流式方式学习哈希函数。

关于数据流大小（即$n_t$）的详细分析见第4.3.1节。



#### 4.1.2 验证

按照前面的方法[35]，[36]，我们采用以下方案来评估性能：平均精度（表示为mAP）、以每个查询为中心的半径为2的汉明球内的精度（表示为Precision@H2)，mAP与不同大小的训练实例曲线及其在mAP曲线下的相应区域（表示为AUC），前K条检索邻居曲线的精度（表示为Precision@K)以及它们在Precision@K曲线下的面积（表示为AUC）。

对于Precision@K，在所有基准上，K的值都在$\{1,5,10,20,30，…，90,100\}$范围内，因为$K≤ 100$是常见的选择，在调查搜索结果时更接近用户行为。在CIFAR-10和MNIST上,关于mAP与不同大小的训练实例的对比，当训练实例的大小在$\{2K、4K、6K、…、18K、20K\}$时，对实验结果进行采样，并且由于Places205规模较大，在Places205为$\{5K、10K、…、95K、100K\}$时，对实验结果进行采样。此类设置与最先进的方法（即BSODH[36]）进行了公平的比较，其中，在CIFAR-10、MNIST和Places205上，流数据大小的最佳选择分别为2K、2K和5K。

值得注意的是，当报告Places205上的mAP性能时，遵循[33]、[35]、[36]中的工作，我们只计算前1000个检索项目（表示为mAP@1000），因为其规模大且耗时长。使用在8、16、32、48、64和128之间变化的哈希位来评估上述度量。

#### 4.1.3 基准

​		为了说明所提出的FCOH的有效性，我们将我们的方法与几种最先进的在线散列算法进行了比较，包括在线内核散列（OKH）[24]、在线草图散列（SketchHash）[26]、自适应散列（AdaptiveHash）[27]、在线监督散列（OSH）[30]、在线互信息散列（MIHash）[33]，基于Hadamard码本的在线哈希（HCOH）和在线离散哈希的平衡相似性（BSODH）[36]。这些方法的源代码是公开的。我们的模型是用MATLAB实现的。实验在一台配备3.60GHz Intel Core I7 4790 CPU和16G RAM的服务器上进行。

#### 4.1.4 设置

为了在所有三个基准上共享相同的数据集配置，我们直接采用[35]、[36]中所述的参数，这些参数已针对每种方法进行了仔细验证。下面详细介绍了参数化设置。

- **OKH**：在CIFAR-10、Places205和MNIST上，元组（C，α）分别设置为（0.001,0.3）、（0.0001,0.7）和（0.001,0.3）
- **SketchHash**：元组（sketchsize, batchsize）在CIFAR-10、Places205和MNIST上分别设置为（200,50）、（100,50）和（200,50）。
- **AdapteHash**：在CIFAR-10、Places205和MNIST上，元组（α、λ、η）分别设置为（0.9、0.01、0.1）、（0.9、0.01、0.1）和（0.8、0.01、0.2）。
- **OSH**：在所有数据集上，η设置为0.1，ECOC码本C的填充方式与[30]中相同。
- **MIHash**：在CIFAR-10、Places205和MNIST上，元组（θ、R、A）分别设置为（0、1000、10）、（0、5000、10）和（0、1000、10）。
- **HCOH**：在CIFAR-10、Places205和MNIST上，元组$（n_t，η）$分别设置为（1,0.2）、（1,0.1）和（1,0.2）。
- **BSODH**：在CIFAR-10、Places205和MNIST上，元组$（λ，σ，η_s，η_d）$分别设置为（0.6,0.5,1.2,0.2），（0.3,0.5,1.0,0.0）和（0.9,0.8,1.2,0.2）。

关于每种方法的这些参数的具体描述分别见[24]、[26]、[27]、[30]、[33]、[35]、[36]。表2显示了建议FCOH的参数设置，经过仔细调整和验证。值得强调的是，对于SketchHash[26]，每个数据流中的训练大小必须大于代码长度。按照[35]、[36]中先前的方法，我们只报告了哈希位为8、16、32、48的实验结果。所有实验均进行了三次，并报告了平均结果。

表2三个基准上的参数配置。

![image-20211206101722112](../note/picture/image-20211206101722112.png)

### 4.2 结果和讨论

#### 4.2.1 CIFAER-10的结果

**表3**

在CIFAR-10上对8、16、32、48、64和128位的mAP和Precision@H2的结果进行比较。最好的结果用黑体字标注，其次是下划线。

![image-20211206104418629](../note/picture/image-20211206104418629.png)

​		关于mAP和Precision@H2，可以找到三个观察结果：首先，如表3所示，FCOH在大多数情况下都能达到最佳效果。从数量上看，FCOH比HCOH或BSODH等最先进的方法平均高出1.49%mAP和7.94%Precision@H2，这证明了FCOH的有效性。其次，对于大多数方法，mAP随着代码长度的增加而增加。在16位的情况下，BSODH增加了很多，甚至比建议的FCOH更好。三是与mAP不同,，Precision@H2大多数方法在高位（64位和128位）时会下降很多。为了分析第二个和第三个观测值之间的对比度，可以将更多的信息编码为高位，这有利于线性扫描 如mAP，同时要检查的不同哈希桶的数量随着散列位r的增加而快速增加，这对直接返回汉明半径2内的数据点的检索场景有害，即Precision@H2 [18].

![image-20211206105120441](../note/picture/image-20211206105120441.png)

**图4**在CIFAR-10上不同大小训练实例的mAP性能。

![image-20211206105203555](../note/picture/image-20211206105203555.png)

**图5**在CIFAR-10上几个进行比较算法的Precision@K 曲线

![image-20211206105329179](../note/picture/image-20211206105329179.png)

**图6**在CIFAR-10上mAP和Precision@K的AUC曲线

我们进一步分析了不同大小训练实例的mAP曲线以及图4和图6（a）中相应的AUC曲线，它们反映了我们对在线学习的适应性。在图4中可以发现两个观察结果：首先，FCOH在不同的训练阶段取得了更好的表现。为了进行深入分析，图6（a）中FCOH的AUC曲线比次优（HCOH）好8.63%，这表明了FCOH的稳健性。其次，FCOH在不同代码长度的早期训练阶段（数据量为2000）获得了更好的性能，这有效地验证了FCOH的在线自适应性。

Precision@K的结果以及它们在CIFAR-10上的AUC曲线见图5和图6（b）。通常，FCOH在所有代码长度中都显示出最佳性能，当代码长度$≥48$ 优势更加明显。关于Precision@K的AUC曲线，如图9（b）中，FCOH的平均值超过次优值（例如MIHash）6.11%。值得注意的是，结合在图5的Precision@K结果和表3中的mAP结果，我们可以得出结论，给定一个查询，建议的HCOH不仅检索更多相关项目，而且还将它们排在首位。

#### 4.2.2 Places205上的结果

**表4**

在Places205 上对8、16、32、48、64和128位的mAP和Precision@H2的结果进行比较。最好的结果用黑体字标注，其次是下划线。

![image-20211206110632131](../note/picture/image-20211206110632131.png)

​		Places205是一个大规模且具有挑战性的基准，其结果可以很好地反映现实世界应用程序中的性能。我们首先分析在表4中的mAP@1000及Precision@H2。可以观察到，FCOH不仅排名第一，而且在mAP@1000上还获得了3.84%的相对增长及在Precision@H2增长了12.51%，与最佳基线HCOH或BSODH进行比较，这证明了所提出的方法对于大规模应用的可扩展性。

此外，在CIFAR-10上的观测结果也可以在Places205找到，比如更好的mAP和Precision@H2在更长的代码长度上的退化。解释与第4.2.1节相同，即较长的代码有利于mAP的线性扫描，但对Precision@H2有害，它直接返回汉明半径为2的数据点。此外，比较表3和表4，对于所有方法，mAP和Precision@H2在Places205上的性能远低于CIFAR-10上的性能。分析一下，这是由于Places205的规模很大。

在这样的基准上获得高性能是相当具有挑战性的。然而，就不同方法之间的比较而言，FCOH在所有测试中都是最好的，这验证了FCOH在实际应用中的可行性。

![image-20211206111323789](../note/picture/image-20211206111323789.png)

**图7**在 Places205上不同大小训练实例的mAP性能

![image-20211206111455268](../note/picture/image-20211206111455268.png)

**图8**在 Places205上几个进行比较算法的Precision@K 曲线

![image-20211206111505117](../note/picture/image-20211206111505117.png)

**图9**在 Places205上mAP和Precision@K的AUC曲线

鉴于在图7中mAP@1000的调查结果，我们可以看到FCOH在所有情况下都表现最好。如图9（a）所示，当训练进行时，它证明了FCOH的鲁棒性，其中FCOH与HCOH相比平均获得18.14%的AUC改善。此外，它还显示了FCOH的快速在线自适应性，因为所提出的方法在早期训练阶段（数据量为5000）增长速度快，并且大大优于其他方法。请注意，当哈希位的范围从8到64时，FCOH显示出比其他方法更好的余量。然而，在128位的情况下，FCOH和BSODH表现出类似的性能（FCOH稍好）。为了解释，如第4.2.1节所述，mAP随着代码长度的增加而增加。当代码长度为128时，其他方法也可以获得相对较高的结果。然而，在所有代码长度中，所提出的方法始终显示出最佳性能，这进一步证明了FCOH的优越性。最后一点是图8中Precision@K的性能和图9（b）中的AUC结果再次证明了FCOH的有效性。可以观察到，在这个大规模且具有挑战性的数据集上，FCOH在图8中获得了更高的精度结果，并超越了最先进的MIHash，AUC增益为3.78%。连同在图7中的mAP@1000，我们可以得出结论，给定一个查询，FCOH不仅检索更多相关项，而且将它们排在首位，这非常符合实际应用的需要。

#### 4.2.3 MNIST 的结果

**表5**

在MNIST上对8、16、32、48、64和128位的mAP和Precision@H2的结果进行比较。最好的结果用黑体字标注，其次是下划线。

![image-20211206112113444](../note/picture/image-20211206112113444.png)

​		在表5中，除了16mAP和128位的Precision@H2（FCOH排名第二）的情况外，所提出的方法得到了最好的结果。表5（MNIST上）中的实验结果与表3（CIFAR-10上）中的实验结果相似。一方面，MNIST和CIFAR-10上的定量值远大于Places205上的定量值。另一方面，对于16位的mAP，MIHash在MNIST和CIFAR-10上都获得了最好的结果。依据Precision@H2，在高位中，BSODH在MNIST（64位）和CIFAR-10（128位）上都获得最佳性能。这是因为MNIST和CIFAR-10具有相似的定量量表（60K）∼ 70K）和类别（两者各10个），这是两个相对简单的基准，与Places205规模（数百万个样本和205个类别）相比。尽管如此，FCOH的整体性能仍然较好。总体而言，提出的FCOH达到了最先进的效果，通过与次优方案的比较，FCOH的在mAP和Precision@H2上平均增长率分别为1.76%和1.08%，证明了FCOH的优越性。

![image-20211206113045875](../note/picture/image-20211206113045875.png)

**图10**在  MNIST上不同大小训练实例的mAP性能

![image-20211206113056183](../note/picture/image-20211206113056183.png)

**图11**在 MNIST上几个进行比较算法的Precision@K 曲线

![image-20211206113105034](../note/picture/image-20211206113105034.png)

**图12**在  MNIST.上mAP和Precision@K的AUC曲线

​		此外，我们认为图10和图12（a）也证明了FCOH的在线适应性。首先，FCOH在不同规模的训练实例中始终保持较好的mAP，与最佳基线HCOH相比，AUC增加9.83%。另一方面，FCOH在早期训练阶段大大超越了其他方法。以64的散列位为例，当训练大小为2000时，FCOH得到的映射为0.689，而MIHash只有0.323，HCOH只有0.329，BSODH只有0.270。因此，FCOH的快速自适应能力得到了很好的证明。

至于Precision@K，如图11所示，FCOH在具有不同代码长度的所有情况下仍然具有竞争力。从数量上讲，FCOH与第二好的MIHash相比，AUC增加了1.48%。此外，与CIFAR-10和Places205上的结果不同，MNIST上的增量较小。分析而言，MNIST是一个简单的基准，现有方法很容易获得高性能。当代码长度≤ 16现有的方法可以达到80%以上的准确率。当代码长度≥32 现有方法可获得接近90%的高精度。因此，很难在MNIST基础上进一步获得显著的改进。

#### 4.2.4 对更好性能的深入分析

如第4.2.1节、第4.2.2节和第4.2.3节所述，FCOH的总体性能优于SOTA。为了分析，类更新侧重于最小化类内距离，并将其他类从当前优化的类中拉出来，从而快速适应新的数据流，如Sec3.2.1所示。半松弛优化将部分二进制约束视为常数，由上一轮学习的哈希权重计算。因此，它可以使用新的数据流更新哈希模型，同时保留过去的信息，如3.2.2小节所示。因此，FCOH的更好性能是类更新和半松弛优化的共同努力的结果。

通过去除类更新和半松弛，我们的FCOH与BSODH相同[36]，它可以作为基线来显示所提出的类更新和半松弛的有效性。为此，引入了两种FCOH变体。第一种变体表示为FCOHS，其中来自所有类的数据都参与了每轮培训，并且保留了建议的半松弛。第二个变量表示为FCOHC，其中保持类更新，放弃半松弛。相反，我们采用[9]中开发的离散循环坐标下降（DCC），如第3.2.2节所述。DCC也是BSODH的标准优化器[36]。然后，我们以32位为例，展示了mAP（@1000）在三个数据集上的性能。表6列出了实验结果。

**表6**

在三个数据集上所提出方法的不同变体的32位结果mAP（@1000）。

![image-20211206114127134](../note/picture/image-20211206114127134.png)

​		可以看出，包含类更新和半松弛优化的FCOH显示了最佳结果，其变体，即FCOHS和FCOHC获得较少的mAP（@1000）性能。其中，无类更新和半松弛优化的BSODH性能最低。因此，它很好地验证了上述分析，即FCOH的更好结果是所提出的类更新和半松弛优化的共同努力。

#### 4.2.5 类更新的收敛性

![image-20211206114424834](../note/picture/image-20211206114424834.png)

**图13**当哈希位为32时，与$$n_t、λ_1、λ_2$$和µ的变化值相关的mAP（@1000）。

![image-20211206114546255](../note/picture/image-20211206114546255.png)

**图14**具有/不具有类更新的可视化。$$W_1$$是起点。在不进行类更新的情况下，更新后的权值位于$W_2$，而在进行类更新的情况下，哈希权值逐梯度更新。它首先使用来自第一个类的数据更新哈希函数，第一个类返回$W_3$，根据$W_3$使用第二个类的数据，然后获得$W_4$。可以看出，最终的$W_4$了比$W_2$得到了更好的解决方案，而无需进行类更新。

​	在图3中，我们举例说明了一个小型示例，它表明类更新可以获得更好的解决方案。在本节中，我们在图14中可视化了等式3的更新，包括/不包括类更新，以进行严格的演示。我们使用两类合成数据实现图14，并使用DCC[9]更新哈希码，以排除半松弛优化的影响。可以看出，使用类方式更新比不使用类方式更新可以获得更优的解决方案，这表明类方式更新为在线哈希学习提供了更有效的工具。

### 4.3 消融实验

在本小节中，我们对FCOH的超参数进行了消融研究，包括第3.1节中定义的流数据的批量大小，即$n_t$；等式（8）中定义的平衡参数，即$λ_1$和$λ_2$；以及学习率µ，如等式（14）中所定义。在不丧失普遍性的情况下，我们使用这些超参数相对于mAP的不同值进行实验(mAP@1000）在图13中是32位的情况下（本文中使用的详细值见表2）。在其他哈希位中也可以观察到类似的实验结果。

#### 4.3.1 $n_t$ 的影响

​		我们首先从分析t阶段批的大小$n_t$的变化值开始。实验样本的$n_t$范围为{100，200，…，2000}，结果如图13（a）所示。可以观察到，在CIFAR-10和MNIST上(mAP@1000）随着批量$n_t$的增加而退化，而在Places205上，mAP(mAP@1000）在$n_t≈ 1, 000$时达到最优. 在实验中，我们根据经验将CIFAR-10和MNIST上的$n_t$值设置为100，将Places205上的$n_t$值设置为1000，如表2所示。这些设置符合在线学习的要求，因为在我们的实验中为$n_t \ll n$（在CIFAR-10和MNIST上为n=20K，在Places205上为n=100K）。

​		我们观察到，对于CIFAR-10和MNIST，它们具有相似的数据大小，并且共享相同的批量大小。至于Places205，它的数据量比CIFAR-10和MNIST大得多，所需的批处理量也比其他批处理量大。为了说明这一点，Places205是一个大规模的检索集。设置较小的值会使捕获和适应数据属性变得困难。这三个数据集代表了现实生活中的不同场景。这些不同的场景通常需要不同的配置来部署模型。因此，根据直觉，不同的数据集具有不同的最佳$n_t$值。

#### 4.3.2 $\lambda_1$ 的影响

​		如式（13）所示，$λ_1$用于反映基于内积的学习方案中相似项目的重要性。图13（b）绘制了$λ_1$的不同值对性能的影响。一般来说，当CIFAR-10上的$λ_1=0.01$、Places205和MNIST上的$λ_1=0.1$时，FCOH在三个基准上获得最佳mAP(mAP@1000)（CIFAR-10为0.702，Places205为0.267，MNIST为0.786）。此外，如图13（b）所示，当$λ_1=0$时，FCOH会遭受巨大的性能损失。更具体地说，在本例中，mAP(mAP@1000)在CIFAR-10、Places205和MNIST上的得分分别为0.565、0.099和0.605。为了分析，当$λ_1=0$时，FCOH仅依赖不同的项来学习哈希函数。待学习的二进制代码将彼此分离。因此，性能退化很多。在实验中，我们根据经验在CIFAR-10上将$λ_1$的值设置为0.01，在Places205和MNIST上将$λ_1$的值设置为0.1。

#### 4.3.3 $\lambda_2$ 的影响

​		如式（13）所示，$λ_2$用于反映基于内积的学习方案中不相似项目的重要性。从图13（c）可以看出，当CIFAR-10和MNIST上的$λ_2=0.01$，Places205上的$λ_2=0.1$时，FCOH获得最佳映射(mAP@1000)分别为0.702、0.267和0.786。与$λ_1$类似，当$λ_2=0$时，FCOH也会遭受巨大的性能损失（CIFAR-10上为0.554，places205处为0.188，MNIST上为0.637）。然而，这种现象背后的解释与$λ_1=0$的情况不同。具体来说，当λ2=0时，所提出的FCOH只通过相似项学习哈希函数，在这种情况下，所学习的二进制码将尽可能接近，这不可避免地导致性能下降。在实验中，我们根据经验在CIFAR-10和MNIST上将λ2的值设置为0.01，在Places205上设置为0.1。

​		结合第4.3.2节和第4.3.3节，元组$（λ_1，λ_2）$的最佳值在CIFAR-10上分别为（0.01,0.01），在Places205上为（0.1,0.1），在MNIST上为（0.1,0.01）。注意，上面的元组集显示了相似项和不同项之间相等（或近似）的重要性，因为$λ1$和$λ2$在三个基准上共享相同（或相似）的值。也就是说，所提出的类更新策略能够很好地解决“数据不平衡”的问题，因为在我们的框架中相似信息和不相似信息都被同等地学习。

#### 4.3.4 $\mu$ 的影响

​		最后，学习率µ的影响如图13（d）所示. 通常的，mAP(mAP@1000)对变化的µ敏感。以获得最佳mAP(mAP@1000)，在实验中，我们根据经验将CIFAR-10、Places205和MNIST上的µ值分别设置为0.01、0.0001和0.1。

​		除了观察到如第4.3.3节所述$λ_1≈ λ_2$，我们经验发现$λ_1、λ_2$和$µ$的合适值在$10^{−m}$，其中m=1,2,3,4,5。因此，在实际应用中，用户可以尝试这些不同的值并选择最佳值。

### 4.4 训练效率

**表7**

32位哈希码下三种基准测试中哈希函数更新和哈希表更新的时间消耗

![image-20211206154149680](../note/picture/image-20211206154149680.png)

​		为了定量评估FCOH的训练效率，包括更新哈希函数和哈希表，我们在表7中给定哈希位$r=32$进行了实验。在其他代码长度中也可以找到类似的观察结果。

**哈希函数更新**。一般来说，OKH和SketchHash是最有效的，但是它们的mAP很差(mAP@1000). 与最先进的方法（MIHash和BSODH）相比，FCOH的效率提高了很多。正如第1节所分析的，MIHash必须为每个实例计算邻居和非邻居之间的汉明距离，而BSODH中采用的离散优化带来了更多变量，并且存在收敛问题。与HCOH相比，该方法在CIFAR-10和MNIST上的训练时间仍然较短，而在Places205上的训练效率更高。为了进行分析，FCOH要求在Places205的批大小更大（表2中的$n_t=2000$），而HCOH的批大小则为$n_t=1$。然而，在实验中，HCOH的性能较差，尤其是在低哈希位方面。可以观察到，在CIFAR-10和Places205上，FCOH比在MNIST上花费更多的训练时间。我们认为这是由于CIFAR-10的高特征维数（4096 维）和Places205（100000）的大规模。

**哈希表更新**。我们进一步比较了哈希表更新的时间消耗。每次更新哈希函数时都需要更新哈希表。很容易知道，哈希表更新所花费的总时间取决于更新哈希函数的次数，即$\frac{n}{n_t}$，其中n是数据集的大小，$n_t$是每个更新阶段的图像批的大小。从表7可以看出，OKH和HCOH在哈希表更新中效率最高。这是因为它们的批处理大小为1，这意味着需要更多的哈希表更新。注意，MIHash还需要$n_t=1$，但是，它在更新哈希表时消耗的时间更少。要进行分析，MIHash不会在每次更新哈希函数时更新哈希表。相反，它设计了一种“选通”机制，仅当更新的哈希模型的互信息优于当前最佳哈希模型时，才会更新哈希表。因此，MIHash需要更少的哈希表更新次数。此外，我们观察到BSODH需要最少的哈希表更新时间。深入分析，BSODH的批量最大（CIFAR-10和MNIST的$n_t=2000$，Places205的$n_t=5000$）。最后，FCOH的优点还体现在其高效的哈希表更新上，如表7所示，尤其优于选通设计的MIHash。虽然BSODH在哈希表更新方面比FCOH更有效，但由于批量较大，因此在哈希函数更新方面效率较低。

总之，该方法在哈希函数更新和哈希表更新方面都是高效的。值得注意的是，本文的重点是设计一种高效的哈希函数更新方法。然而，一些专门为哈希表更新而设计的现有作品[55]、[56]可以集成到上述在线哈希方法中，以进一步提高哈希表更新的效率。

### 4.5 与离线哈希方法的比较

![image-20211206160109049](../note/picture/image-20211206160109049.png)

**图15**CIFAR-10上每个训练阶段哈希函数更新的mAP性能和时间消耗。

​		在线哈希的主要焦点是有效地处理流数据。传统的离线散列也可以通过使用累积的数据重新训练模型来完成这项工作，然而这种训练消耗非常大，但具有更高的准确性。在本小节中，我们进一步进行实验，以与最先进的deep-MIHash[23]进行比较，后者是MIHash的离线扩展[33]。为此，我们在CIFAR-10上进行了32位实验，并分析了mAP性能及其训练时间，结果如图15所示。对于deep-MIHash，我们用GPU和CPU的训练时间。可以看出，deep-MIHash可以获得更好的mAP性能，即在每个更新阶段比FCOH提高10%左右。然而，与使用GPU或CPU的deep-MIHash相比，FCOH在更新哈希函数时的时间消耗减少了一个数量级。为了进行分析，deep-MIHash必须累积所有可用数据以重新训练哈希函数。因此，它避免了过去流数据的信息丢失。然而，训练时间也随着流数据的增加而线性增长。

### 4.6 限制和今后的工作

​		与另一种基于内积的方法BSODH相比，FCOH是一种基于内积的方法，具有两个创新点，即类更新和半松弛优化。它有三个优点，即至少75%存储节省（见第3.2.1节）、更高的准确性和快速在线自适应（见第4.2节）以及高效训练（见第4.4节）。然而，由于类更新方案的设计，它只能处理单标签数据，这需要相互排斥的类标签。这种严格的要求在许多情况下可能无法满足，例如，多标签数据集，这可能会限制其在某些现实应用中的应用。在今后的工作中，我们将进一步努力解决这个问题。

## 5 结论

​		在本文中，我们提出了一种新的有监督在线哈希方法，称为FCOH，这是一种基于内积的方案，它解决了现有在线哈希方法中普遍存在的在线适应性差和训练效率低的问题。为此，我们首先开发了一种类更新方案，该方案在每个阶段迭代更新哈希函数，通过该方案，FCOH可以在更少的训练数据和存储消耗的情况下获得更好的性能。其次，我们提出了一种半松弛优化方法，该方法通过预计算将一部分二元约束松弛，而将另一部分作为一个常数二元矩阵处理，通过预计算不引入额外变量，且时间消耗较少。在三个广泛使用的基准上进行的大量实验表明，FCOH在在线检索任务中实现了最先进的准确性和效率。

